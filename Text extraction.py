# -*- coding: utf-8 -*-
"""CV 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k-GbZvQQ4mKN3TZh4lOqhpaO4povqlon

**Text extraction **:
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install textract
!apt install tesseract-ocr
!apt-get install tesseract-ocr-eng
!apt-get install tesseract-ocr-fra
!python -m spacy download en_core_web_lg
!python -m spacy download fr_core_news_lg
!pip install pytesseract
import nltk
nltk.download('punkt')
import nltk
nltk.download('stopwords')
import numpy as np
import os
import re
import spacy
import string
import textract
import pandas as pd
import seaborn as sns
import matplotlib.pylab as pylab
from matplotlib import pyplot as plt
import glob

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

import warnings
warnings.filterwarnings('ignore')
from google.colab import drive;
drive.mount('/content/drive');

sns.set_style('darkgrid')
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

"""acces aux réparatoires"""

chemins = glob.glob('/content/drive/MyDrive/cvs/*')
noms_des_dossiers = [os.path.basename(chemin) for chemin in chemins]
print(noms_des_dossiers)

"""Extract text from dataset"""

file_path1 = []
category1  = []
directory1 = '/content/drive/MyDrive/cvs/Data science'
for i in os.listdir(directory1):
    if i.endswith('.jpg'):
        os.path.join(directory1, i)
        file_path1.append((textract.process(os.path.join(directory1, i))).decode('utf-8'))
        category1.append('Data science')

data1 = pd.DataFrame(data = file_path1 , columns = ['Raw_Details'])
data1['Category1'] = category1
data1

file_path2 = []
category2  = []
directory2 = '/content/drive/MyDrive/cvs/Electronique et systéme embarqués'
for i in os.listdir(directory2):
    if i.endswith('.jpg'):
        os.path.join(directory2, i)
        file_path2.append((textract.process(os.path.join(directory2, i))).decode('utf-8'))
        category2.append('Electronique et systéme embarqués')

data2 = pd.DataFrame(data = file_path2 , columns = ['Raw_Details'])
data2['Category2'] = category2
data2

file_path3 = []
category3  = []
directory3 = '/content/drive/MyDrive/cvs/Génie logiciel'
for i in os.listdir(directory3):
    if i.endswith('.jpg'):
        os.path.join(directory3, i)
        file_path3.append((textract.process(os.path.join(directory3, i))).decode('utf-8'))
        category3.append('Génie logiciel')

data3 = pd.DataFrame(data = file_path3 , columns = ['Raw_Details'])
data3['Category3'] = category3
data3

file_path4 = []
category4  = []
directory4 = '/content/drive/MyDrive/cvs/IOT'
for i in os.listdir(directory4):
    if i.endswith('.jpg'):
        os.path.join(directory4, i)
        file_path4.append((textract.process(os.path.join(directory4, i))).decode('utf-8'))
        category4.append('IOT')

data4 = pd.DataFrame(data = file_path4 , columns = ['Raw_Details'])
data4['Category4'] = category4
data4

file_path5 = []
category5  = []
directory5 = '/content/drive/MyDrive/cvs/mecanique'
for i in os.listdir(directory5):
    if i.endswith('.jpg'):
        os.path.join(directory5, i)
        file_path5.append((textract.process(os.path.join(directory5, i))).decode('utf-8'))
        category5.append('mecaique')

data5 = pd.DataFrame(data = file_path5 , columns = ['Raw_Details'])
data5['Category5'] = category5
data5

file_path6 = []
category6  = []
directory6 = '/content/drive/MyDrive/cvs/technologies de linformation'
for i in os.listdir(directory6):
    if i.endswith('.jpg'):
        os.path.join(directory6, i)
        file_path6.append((textract.process(os.path.join(directory6, i))).decode('utf-8'))
        category6.append('technologies de l information')

data6 = pd.DataFrame(data = file_path6 , columns = ['Raw_Details'])
data6['Category6'] = category6
data6

file_path7 = []
category7  = []
directory7 = '/content/drive/MyDrive/cvs/télécommunications'
for i in os.listdir(directory7):
    if i.endswith('.jpg'):
        os.path.join(directory7, i)
        file_path7.append((textract.process(os.path.join(directory7, i))).decode('utf-8'))
        category7.append('télécommunications')

data7 = pd.DataFrame(data = file_path7 , columns = ['Raw_Details'])
data7['Category7'] = category7
data7

"""create a dataframe"""

resume_data = data1.append([data2, data3, data4,data5, data6, data7 ], ignore_index = True)
resume_data

resume_data.info()

resume_data['Category'] = resume_data.apply(lambda row: ', '.join(row[['Category1', 'Category2', 'Category3', 'Category4', 'Category5', 'Category6', 'Category7']].dropna()), axis=1)
resume_data

resume_data.drop(['Category1', 'Category2', 'Category3', 'Category4', 'Category5', 'Category6', 'Category7'], axis = 1, inplace = True)
resume_data = resume_data[["Category", "Raw_Details"]]

"""Final dataset"""

resume_data.head(100)

resume_data.to_csv('Raw_Resume.csv', index=False)

resume_data = pd.read_csv("Raw_Resume.csv")
resume_data

"""Data Understanding"""

from nltk.corpus import stopwords

# Charger les stopwords en anglais
stop_english = set(stopwords.words('english'))

# Charger les stopwords en français
stop_french = set(stopwords.words('french'))

# Combinez les deux ensembles de stopwords
stop = stop_english.union(stop_french)

# Appliquer le comptage des stopwords
resume_data['Stopwords'] = resume_data['Raw_Details'].apply(lambda x: len([word for word in x.split() if word.lower() in stop]))
resume_data[['Raw_Details','Stopwords']].head()

resume_data['Numerics'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
resume_data[['Raw_Details','Numerics']].head()

"""Text Pre-Processing"""

def preprocess(sentence):
    sentence = str(sentence)
    sentence = sentence.lower()
    sentence = sentence.replace('{html}',"")
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', sentence)
    rem_url = re.sub(r'http\S+', '',cleantext)
    rem_num = re.sub('[0-9]+', '', rem_url)
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(rem_num)
    filtered_words = [w for w in tokens if len(w) > 2 if not w in stop]

    return " ".join(filtered_words)

resume_data = pd.read_csv('Raw_Resume.csv')
resume_data['Resume_Details'] = resume_data.Raw_Details.apply(lambda x: preprocess(x))

resume_data

resume_data.drop(['Raw_Details'], axis = 1, inplace = True)
resume_data

resume_data.to_csv('Cleaned_Resumes.csv', index = False)

resume_data = pd.read_csv('Cleaned_Resumes.csv')
resume_data

"""Named Entity Recognition (NER)"""

#afficher les mots les plus fréquents
oneSetOfStopWords = stop_english.union(stop_french).union(['``', "''"])

totalWords = []
Sentences = resume_data['Resume_Details'].values
cleanedSentences = ""

for records in Sentences:
    cleanedText = preprocess(records)
    cleanedSentences += cleanedText
    requiredWords = nltk.word_tokenize(cleanedText)

    for word in requiredWords:
        # Utiliser word.lower() pour rendre la comparaison insensible à la casse
        if word.lower() not in oneSetOfStopWords and word not in string.punctuation:
            totalWords.append(word)

wordfreqdist = nltk.FreqDist(totalWords)
mostcommon = wordfreqdist.most_common(50)
print(mostcommon)

"""Parts Of Speech (POS) Tagging

"""

from spacy import displacy

# Charger les modèles spaCy
nlp = spacy.load('en_core_web_lg')
# Sélectionner un bloc de texte
one_block = cleanedSentences[1300:5200]
# Appliquer les modèles spaCy aux blocs de texte
doc_block_en = nlp(one_block)
# Afficher les entités nommées pour le modèle en anglais
displacy.render(doc_block_en, style='ent', jupyter=True)

nlp_fr = spacy.load('fr_core_news_lg')
one_block = cleanedSentences[1300:5200]
doc_block_fr = nlp_fr(one_block)
# Afficher les entités nommées pour le modèle en français
displacy.render(doc_block_fr, style='ent', jupyter=True)



"""**Model building **"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, classification_report, precision_score, recall_score


import warnings
warnings.filterwarnings('ignore')

resume_data = pd.read_csv('Cleaned_Resumes.csv')
resume_data.head()

resume_data_encoded = pd.get_dummies(resume_data, columns=['Category'], prefix='Category')

# Affichez les premières lignes du DataFrame après l'encodage
print("\nAprès l'encodage :")
print(resume_data_encoded.head())

resume_data.describe()

resume_data.isnull().sum()

x = resume_data['Resume_Details'].values
y = resume_data['Category'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=45, test_size=0.25, stratify=None)
x_train.shape, x_test.shape

tfidf_vector = TfidfVectorizer(sublinear_tf=True,stop_words='english')

x_train = tfidf_vector.fit_transform(x_train)
x_test = tfidf_vector.transform(x_test)

x_train.shape, x_test.shape

print(y_train),print(y_test)

""" KNN Classifier

"""

model_knn = KNeighborsClassifier(n_neighbors=41)
model_knn.fit(x_train, y_train)
y_pred = model_knn.predict(x_test)
accuracy_knn = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_knn.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_knn.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_knn,classification_report(y_test, y_pred)))
nb_score = model_knn.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_knn = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_knn = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_knn = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_knn = round(accuracy_score(y_test,y_pred),2)

model_DT = DecisionTreeClassifier(criterion='gini')
model_DT.fit(x_train, y_train)
y_pred = model_DT.predict(x_test)
accuracy_DT = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_DT.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_DT.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_DT,classification_report(y_test, y_pred)))
nb_score = model_DT.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_DT = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_DT= round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_DT = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_DT = round(accuracy_score(y_test,y_pred),2)

model_RF = RandomForestClassifier(n_estimators=200)
model_RF.fit(x_train, y_train)
y_pred = model_RF.predict(x_test)
accuracy_RF = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_RF.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_RF.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_RF,classification_report(y_test, y_pred)))
nb_score = model_RF.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)


precision_RF = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_RF = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_RF = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_RF = round(accuracy_score(y_test,y_pred),2)

model_svm = SVC()
model_svm.fit(x_train, y_train)
y_pred = model_svm.predict(x_test)
accuracy_svm = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_svm.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_svm.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_svm,classification_report(y_test, y_pred)))
nb_score = model_svm.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_svm = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_svm = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_svm = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_svm = round(accuracy_score(y_test,y_pred),2)

model_lgr = LogisticRegression()
model_lgr.fit(x_train, y_train)
y_pred = model_lgr.predict(x_test)
accuracy_lgr = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_lgr.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_lgr.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_lgr,classification_report(y_test, y_pred)))
nb_score = model_lgr.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)


precision_lgr = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_lgr = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_lgr = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_lgr = round(accuracy_score(y_test,y_pred),2)

model_bagg = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)
model_bagg.fit(x_train, y_train)
y_pred = model_bagg.predict(x_test)
accuracy_bagg = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_bagg.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_bagg.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_bagg,classification_report(y_test, y_pred)))
nb_score = model_bagg.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_bagg = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_bagg = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_bagg = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_bagg = round(accuracy_score(y_test,y_pred),2)

model_Adaboost = AdaBoostClassifier(n_estimators=100)
model_Adaboost.fit(x_train, y_train)
y_pred = model_Adaboost.predict(x_test)
accuracy_Adaboost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_Adaboost.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_Adaboost.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_Adaboost,classification_report(y_test, y_pred)))
nb_score = model_Adaboost.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Adaboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Adaboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Adaboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Adaboost = round(accuracy_score(y_test,y_pred),2)

model_GradientBoost = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)
model_GradientBoost.fit(x_train, y_train)
y_pred = model_GradientBoost.predict(x_test)
accuracy_GradientBoost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_GradientBoost.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_GradientBoost.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_GradientBoost,classification_report(y_test, y_pred)))
nb_score = model_GradientBoost.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Gradientboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Gradientboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Gradientboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Gradientboost = round(accuracy_score(y_test,y_pred),2)

model_NB =MultinomialNB(alpha=1, fit_prior=False, class_prior=None)
model_NB.fit(x_train, y_train)
y_pred = model_NB.predict(x_test)
accuracy_NB = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_NB.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_NB.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_NB,classification_report(y_test, y_pred)))
nb_score = model_NB.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_NB = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_NB = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_NB = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_NB = round(accuracy_score(y_test,y_pred),2)

Evaluation = {"Models":["KNN Classifier","DecisionTree Classifier","RandomForest Classifier","SVM Classifier",
                        "Logistic Regression","Bagging Classifier","AdaBoost Classifier","Gradient Boosting Classifier","Naive Bayes Classifier"],\
            "Train_Accuracy(%)":[model_knn.score(x_train, y_train),model_DT.score(x_train, y_train),model_RF.score(x_train, y_train),model_svm.score(x_train, y_train),model_lgr.score(x_train, y_train),
                                 model_bagg.score(x_train, y_train),model_Adaboost.score(x_train, y_train),model_GradientBoost.score(x_train, y_train), model_NB.score(x_train, y_train)],
            "Test_Accuracy(%)":[accuracy_knn,accuracy_DT,accuracy_RF,accuracy_svm,accuracy_lgr,accuracy_bagg,accuracy_Adaboost,accuracy_GradientBoost,accuracy_NB],\
            "Precision(%)":[precision_knn,precision_DT,precision_RF,precision_svm,precision_lgr,precision_bagg,precision_Adaboost,precision_Gradientboost,precision_NB],\
            "Recall(%)":[recall_knn,recall_DT,recall_RF,recall_svm,recall_lgr,recall_bagg,recall_Adaboost,recall_Gradientboost,recall_NB],\
            "F1-Score(%)":[f1_knn,f1_DT,f1_RF,f1_svm,f1_lgr,f1_bagg,f1_Adaboost,f1_Gradientboost,f1_NB]}
table = pd.DataFrame(Evaluation)
table

import pickle
filename = 'model_GradientBoost.pkl'
pickle.dump(model_GradientBoost,open(filename,'wb'))

import pickle
filename = 'vector.pkl'
pickle.dump(tfidf_vector,open(filename,'wb'))